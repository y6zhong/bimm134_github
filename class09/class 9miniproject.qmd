---
title: "class09: mini-project"
author: "Chelsea (A16871799)"
format: pdf
---
Today we apply machine learning methods on breast cancer biopsy data from Fine needle aspiration(Fna)

##data input
The data is supplied on CSV format

```{r}
wisc.df <- read.csv("WisconsinCancer.csv", row.names=1)
head(wisc.df)
```
Now I will store the diagnosis column and to exclude it from the data set called wisc.data
```{r}
# We can use -1 here to remove the first column
wisc.data <- wisc.df[,-1]
# Create diagnosis vector for later 
diagnosis <- as.factor(wisc.df$diagnosis)
```

Q1. How many observations are in this dataset?
```{r}
nrow(wisc.data)
```

Q2. How many of the observations have a malignant diagnosis?
```{r}
table(wisc.df$diagnosis)
```
```{r}
sum(wisc.df$diagnosis=="M")
```
Q3. How many variables/features in the data are suffixed with _mean?
```{r}
x <- colnames(wisc.df)
grep("_mean", x) 
```
```{r}
length(grep("_mean",x) )
```

# Check column means and standard deviations

```{r}
colMeans(wisc.data)
apply(wisc.data,2,sd)
```
##Principal Component Analysis
we need to scale our input data before PCA as some of the columns are measured in terms of different units with different means and different variance. The upshot here is we se`scale=true` argument to `prcomp()`
```{r}
wisc.pr <- prcomp(wisc.data, scale=TRUE)
summary(wisc.pr)
```

Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)?
#PC1 capture 0.4427 of the original variance
Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?
#at least 3 principal components are required to describe at least 70% of the original variance in the data?
Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?
#at least 7 Principal components are required to describe at least 90% of the original variance in the data.
Generate a PCA plot 
```{r}
plot(wisc.pr$x[,1],wisc.pr$x[,2],col=diagnosis,pch=16)
```
```{r}
biplot(wisc.pr)
```

Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?

This is a hot mess of a plot (difficult to understand) since. Rownames are used as the plotting character for biplots in here which make trends hard to see.
# Scatter plot observations by components 1 and 2
```{r}
plot( wisc.pr$x, col = diagnosis, 
     xlab = "PC1", ylab = "PC2")
```

```{r}
# Repeat for components 1 and 3
plot(wisc.pr$x[, ], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")
```
Q8. Generate a similar plot for principal components 1 and 3. What do you notice about these plots?
In general, the plots indicate that principal component 1 is capturing a separation of malignant (red) from benign (black) samples. Principal component 2 explains more variance in the original data than principal component 3, so the plot w PC1 and PC2 has a cleaner cut separating the two subgroups compared with PC2 and PC3
```{r}
# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis

# Load the ggplot2 package
library(ggplot2)

# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```
```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```
```{r}
# Variance explained by each principal component: pve
pve <- pr.var/ sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

install.packages("factoextra")
```{r}
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)
```

Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean?
```{r}
wisc.pr$rotation[,1]
```
concave.points_mean=-0.26085376

Q10. What is the minimum number of principal components required to explain 80% of the variance of the data?
```{r}
summary(wisc.pr)
```
#at least 5 principal components are required to describe at least 80% of the original variance in the data?


```{r}
data.dist <- dist(scale(wisc.data))
wisc.hclust <- hclust(data.dist)
plot(wisc.hclust)
```
Q11. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters?
```{r}
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust,k=4)
table(wisc.hclust.clusters, diagnosis)
```

Q12. Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10?
```{r}
wisc.hclust.better <- cutree(wisc.hclust,k=2)
table(wisc.hclust.better, diagnosis)
```
better cluster vs diagonses match by cutting into 2 clusters.
 "single", "complete", "average" and (my favorite) "ward.D2".

Q13. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.
```{r}
hc_single<-hclust(data.dist, method="single")
plot(hc_single, main="single linkeage")
hc_complete<-hclust(data.dist, method="complete")
plot(hc_complete, main="complete linkeage")
hc_average<-hclust(data.dist, method="average")
plot(hc_average, main="average linkeage")
hc_ward.D2<-hclust(data.dist, method="ward.D2")
plot(hc_ward.D2, main="ward.D2 linkeage")
```
My favorite result is the method="ward.D2" because it creates groups which variance is minimized within clusters. This has the effect of looking for spherical clusters with the process starting with all points in individual clusters (bottom up) and then repeatedly merging a pair of clusters such that when merged there is a minimum increase in total within-cluster variance This process continues until a single group including all points (the top of the tree) is defined.

Q14. How well does k-means separate the two diagnoses? How does it compare to your hclust results?
```{r}
wisc.km <- kmeans(data.dist, centers=2, nstart=20)
table(wisc.km$cluster,diagnosis)
```
```{r}
table (wisc.km$cluster, wisc.hclust.clusters)
```
By looking at the second table, it looks like clusters 1, 2, and 4 from the hierarchical clustering model can be interpreted as the cluster 1 equivalent from the k-means algorithm, and cluster 3 can be interpreted as the cluster 2 equivalent. k-means separate the two diagnoses similarly compared to my hclust results?

Combining methods.
This approach will take not original data our PCA results and work with them.
```{r}
d <- dist(wisc.pr$x[,1:3])
wisc.pr.hclust <- hclust(d,method="ward.D2")
plot(wisc.pr.hclust)
```
generate 2 cluster group from this hclust object
```{r}
grps <- cutree(wisc.pr.hclust,k=2)
grps
```
```{r}
plot(wisc.pr$x[,1],wisc.pr$x[,2],col=grps)
```

```{r}
table(grps)
```
```{r}
table(diagnosis)
```
```{r}
table(diagnosis,grps)
```

Q15. How well does the newly created model with four clusters separate out the two diagnoses?
```{r}
grps4 <- cutree(wisc.pr.hclust,k=4)
table(diagnosis,grps4)
```
Q17. wisc.hclust.better with k=2 resulted in a clustering model with the best specificity.

Q18. Which of these new patients should we prioritize for follow up based on your results?
```{r}
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
```
```{r}
plot(wisc.pr$x[,1:2])
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
```
prioritize cluster 1 based on your results.







